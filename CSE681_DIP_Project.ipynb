{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk_-m6c4Pyvp"
      },
      "source": [
        "# Medical Image Denoising with DDPM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pfJwL9QPwIR"
      },
      "source": [
        "## Device Details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJlnGAeUP6FN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzZ5MM98QA2j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torchvision.transforms import Compose, Resize, Lambda\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "import math\n",
        "from diffusers import DDPMPipeline\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from accelerate import Accelerator\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.restoration import denoise_nl_means, estimate_sigma\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qzsa2-NSgwr"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers transformers accelerate datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ2oyKZ_QuT-"
      },
      "source": [
        "## Dataset Loading and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG3c0BwzQDgA"
      },
      "outputs": [],
      "source": [
        "def load_and_split_dataset(dataset_name=\"iamkzntsv/IXI2D\", train_ratio=0.8, random_seed=42):\n",
        "    \"\"\"\n",
        "    Load ALL splits from the dataset, combine them using concatenate_datasets, then split.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: Name of the Hugging Face dataset\n",
        "        train_ratio: Proportion of data for training (0.8 = 80% train, 20% test)\n",
        "        random_seed: Random seed for reproducible splits\n",
        "\n",
        "    Returns:\n",
        "        full_dataset: Combined dataset from all splits\n",
        "        train_indices: Indices for training data\n",
        "        test_indices: Indices for testing data\n",
        "    \"\"\"\n",
        "    print(f\"Loading dataset: {dataset_name}...\")\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(dataset_name)\n",
        "\n",
        "    # Get each split (using .get() to handle missing splits gracefully)\n",
        "    train_data = dataset.get('train', None)\n",
        "    validation_data = dataset.get('validation', None)\n",
        "    test_data = dataset.get('test', None)\n",
        "\n",
        "    # Count images in each split\n",
        "    train_count = len(train_data) if train_data is not None else 0\n",
        "    validation_count = len(validation_data) if validation_data is not None else 0\n",
        "    test_count = len(test_data) if test_data is not None else 0\n",
        "\n",
        "    # Print the counts\n",
        "    print(f\"Number of images in training set: {train_count}\")\n",
        "    print(f\"Number of images in validation set: {validation_count}\")\n",
        "    print(f\"Number of images in test set: {test_count}\")\n",
        "\n",
        "    # Combine the datasets step by step\n",
        "    full_dataset = None\n",
        "\n",
        "    if train_data is not None:\n",
        "        full_dataset = train_data\n",
        "\n",
        "    if validation_data is not None:\n",
        "        if full_dataset is not None:\n",
        "            full_dataset = concatenate_datasets([full_dataset, validation_data])\n",
        "        else:\n",
        "            full_dataset = validation_data\n",
        "\n",
        "    if test_data is not None:\n",
        "        if full_dataset is not None:\n",
        "            full_dataset = concatenate_datasets([full_dataset, test_data])\n",
        "        else:\n",
        "            full_dataset = test_data\n",
        "\n",
        "    # Check if we have any data\n",
        "    if full_dataset is None:\n",
        "        raise ValueError(\"No data found in any of the splits!\")\n",
        "\n",
        "    print(f\"Total number of images: {len(full_dataset)}\")\n",
        "\n",
        "    # Create indices for splitting\n",
        "    total_samples = len(full_dataset)\n",
        "    indices = list(range(total_samples))\n",
        "\n",
        "    # Split the indices\n",
        "      #shuffle=True + same seed → The split will stay identical every run.\n",
        "      #shuffle=True + different seed → The split will change each run.\n",
        "\n",
        "    train_indices, test_indices = train_test_split(\n",
        "        indices,\n",
        "        train_size=train_ratio,\n",
        "        random_state=random_seed,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    print(f\"Final split - Train: {len(train_indices)}, Test: {len(test_indices)}\")\n",
        "\n",
        "    return full_dataset, train_indices, test_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtUvqRifXwG_"
      },
      "outputs": [],
      "source": [
        "def display_images(dataset, indices, num_images=5, cols=5):\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
        "    axes = np.array(axes).reshape(-1)  # Flatten for easy indexing\n",
        "\n",
        "    for i in range(num_images):\n",
        "        if i < len(indices):\n",
        "            img = dataset[indices[i]]['image']\n",
        "            img_np = np.array(img)\n",
        "\n",
        "            if img_np.ndim == 2:  # true grayscale\n",
        "                axes[i].imshow(img_np, cmap='gray')\n",
        "            elif img_np.ndim == 3 and np.all(img_np[..., 0] == img_np[..., 1]) and np.all(img_np[..., 1] == img_np[..., 2]):\n",
        "                axes[i].imshow(img_np[..., 0], cmap='gray')  # fake RGB but grayscale\n",
        "            else:\n",
        "                axes[i].imshow(img_np)  # real RGB\n",
        "\n",
        "            axes[i].set_title(f'Image {indices[i]}')\n",
        "            axes[i].axis('off')\n",
        "        else:\n",
        "            axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXphnfFoQZnR"
      },
      "outputs": [],
      "source": [
        "def show_image_statistics(dataset, index):\n",
        "    \"\"\"\n",
        "    Display true statistics of an image in the dataset without any modifications.\n",
        "\n",
        "    Args:\n",
        "        dataset: Hugging Face dataset object containing 'image'\n",
        "        index: Index of the image in the dataset\n",
        "    \"\"\"\n",
        "    # Extract as PIL\n",
        "    # PIL = Python Imaging Library (now Pillow) — used to handle images easily in Python.\n",
        "    # Useful in Many transforms (like resize, crop, convert to grayscale) work naturally on PIL images.\n",
        "    # It also makes it easy to inspect image metadata (mode, size, format),\n",
        "    #In addition, it avoids accidentally altering the image’s data type or pixel range before statistics are printed.\n",
        "    image_pil = dataset[index]['image']\n",
        "\n",
        "    # Convert to NumPy without altering mode\n",
        "    image_np = np.array(image_pil)\n",
        "\n",
        "    print(f\"--- Image Statistics (Index: {index}) ---\")\n",
        "    print(f\"Mode          : {image_pil.mode}\")  # e.g., 'RGB', 'L'\n",
        "    print(f\"Shape         : {image_np.shape}\")\n",
        "    print(f\"Data type     : {image_np.dtype}\")\n",
        "    print(f\"Min value     : {image_np.min()}\")\n",
        "    print(f\"Max value     : {image_np.max()}\")\n",
        "    print(f\"Mean value    : {image_np.mean():.4f}\")\n",
        "    print(f\"Std deviation : {image_np.std():.4f}\")\n",
        "    print(f\"Median value  : {np.median(image_np):.4f}\")\n",
        "    print(f\"Unique values : {len(np.unique(image_np))}\")\n",
        "\n",
        "    # Show image in true color if RGB\n",
        "    if image_pil.mode == 'RGB':\n",
        "        plt.imshow(image_np)\n",
        "    else:\n",
        "        plt.imshow(image_np, cmap='gray')\n",
        "\n",
        "    plt.title(f\"Image {index}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDm9gPgLQ7xD"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXdrLpf2Qq2P"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_save_dataset(dataset, indices, target_size=(128, 128), normalize_type=\"unsigned\", save_path=\"/content/preprocessed_dataset.pt\"):\n",
        "    \"\"\"\n",
        "    Preprocess the dataset immediately and save to disk.\n",
        "\n",
        "    Args:\n",
        "        dataset: HuggingFace dataset object containing 'image'\n",
        "        indices: List of indices to preprocess\n",
        "        target_size: (width, height) to resize images\n",
        "        normalize_type:\n",
        "            \"unsigned\" → normalize to [0, 1] range\n",
        "            \"signed\"   → normalize to [-1, 1] range\n",
        "        save_path: Path to save the tensor file\n",
        "    \"\"\"\n",
        "    if normalize_type not in [\"unsigned\", \"signed\"]:\n",
        "        raise ValueError(\"normalize_type must be 'unsigned' (0 –> 1 range) or 'signed' (-1 -> 1 range)\")\n",
        "\n",
        "    print(f\"\\n[INFO] Starting preprocessing of {len(indices)} images...\")\n",
        "    print(f\"[INFO] Target size: {target_size}, Normalization: '{normalize_type}'\")\n",
        "\n",
        "    # Define normalization transformation\n",
        "    if normalize_type == \"unsigned\":\n",
        "        norm_fn = Lambda(lambda img: torch.tensor(np.array(img)).unsqueeze(0).float() / 255.0)\n",
        "    else:  # signed (-1, 1)\n",
        "        norm_fn = Lambda(lambda img: torch.tensor(np.array(img)).unsqueeze(0).float() / 127.5 - 1.0)\n",
        "\n",
        "    transforms = Compose([\n",
        "        Resize(target_size, interpolation=Image.Resampling.BILINEAR),\n",
        "        Lambda(lambda img: img.convert(\"L\")),  # Convert to grayscale\n",
        "        norm_fn\n",
        "    ])\n",
        "\n",
        "    preprocessed_images = []\n",
        "    for idx in indices:\n",
        "        img = dataset[idx][\"image\"]\n",
        "        img_tensor = transforms(img)\n",
        "        preprocessed_images.append(img_tensor)\n",
        "\n",
        "    preprocessed_tensor = torch.stack(preprocessed_images)\n",
        "    torch.save(preprocessed_tensor, save_path)\n",
        "\n",
        "    print(f\"[INFO] Finished preprocessing. Dataset saved to: {save_path}\")\n",
        "    print(f\"[INFO] Tensor shape: {preprocessed_tensor.shape}, dtype: {preprocessed_tensor.dtype}\\n\")\n",
        "    return preprocessed_tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tup7QmbrRCYF"
      },
      "outputs": [],
      "source": [
        "def display_preprocessed_images(tensor, num_images=5, cols=None):\n",
        "    \"\"\"\n",
        "    Display images from preprocessed tensor with adaptive layout.\n",
        "\n",
        "    Args:\n",
        "        tensor: Preprocessed tensor of shape (N, 1, H, W)\n",
        "        num_images: Number of images to display\n",
        "        cols: Number of columns (if None, auto-select)\n",
        "    \"\"\"\n",
        "    # tensor = tensor.cpu() : Moves a tensor from GPU memory to CPU memory as\n",
        "    # it is needed to convert it to a NumPy array (.numpy()), NumPy only works on CPU tensors,\n",
        "    # it is also neededfor plotting or saving, in addition displaying an image in matplotlib (which works on CPU).\n",
        "    # No modification needed for Colab T4 — works the same on GPU or CPU.\n",
        "\n",
        "    tensor = tensor.cpu()\n",
        "\n",
        "    num_images = min(num_images, len(tensor))\n",
        "\n",
        "    if cols is None:\n",
        "        cols = min(num_images, 5)  # auto-limit to max 5 per row\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n",
        "    axes = np.atleast_1d(axes).flatten()\n",
        "\n",
        "    for i in range(len(axes)):\n",
        "        if i < num_images:\n",
        "            img = tensor[i, 0].numpy()\n",
        "            axes[i].imshow(img, cmap='gray')\n",
        "            axes[i].set_title(f\"Image {i}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_preprocessed_statistics(tensor, index):\n",
        "    \"\"\"\n",
        "    Show statistics of a single preprocessed image tensor.\n",
        "    \"\"\"\n",
        "    img = tensor[index, 0].cpu().numpy()\n",
        "\n",
        "    print(f\"--- Preprocessed Image Statistics (Index: {index}) ---\")\n",
        "    print(f\"Shape         : {img.shape}\")\n",
        "    print(f\"Data type     : {img.dtype}\")\n",
        "    print(f\"Min value     : {img.min():.4f}\")\n",
        "    print(f\"Max value     : {img.max():.4f}\")\n",
        "    print(f\"Mean value    : {img.mean():.4f}\")\n",
        "    print(f\"Std deviation : {img.std():.4f}\")\n",
        "    print(f\"Median value  : {np.median(img):.4f}\")\n",
        "    print(f\"Unique values : {len(np.unique(img))}\")\n",
        "\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f\"Preprocessed Image {index}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW_QlST8Roxh"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm_oS2X7RrCR"
      },
      "outputs": [],
      "source": [
        "def get_data_loaders(\n",
        "    train_path=\"/content/train_preprocessed.pt\",\n",
        "    test_path=\"/content/test_preprocessed.pt\",\n",
        "    batch_size=16,\n",
        "    shuffle_train=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Load preprocessed datasets from disk and return train/test DataLoaders.\n",
        "\n",
        "    Args:\n",
        "        train_path: Path to saved preprocessed training tensor\n",
        "        test_path: Path to saved preprocessed testing tensor\n",
        "        batch_size: Batch size for loaders\n",
        "        shuffle_train: Whether to shuffle training data\n",
        "        num_workers: DataLoader num_workers\n",
        "        pin_memory: Whether to use pinned memory (speeds up GPU transfer)\n",
        "\n",
        "    Returns:\n",
        "        train_loader, test_loader\n",
        "    \"\"\"\n",
        "    # Load preprocessed tensors\n",
        "    train_tensor = torch.load(train_path)\n",
        "    test_tensor = torch.load(test_path)\n",
        "\n",
        "    print(f\"[INFO] Loaded train tensor: {train_tensor.shape}\")\n",
        "    print(f\"[INFO] Loaded test tensor: {test_tensor.shape}\")\n",
        "\n",
        "    # Wrap in TensorDataset (no labels for diffusion models)\n",
        "    train_dataset = TensorDataset(train_tensor)\n",
        "    test_dataset = TensorDataset(test_tensor)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,#shuffle_train\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory\n",
        "    )\n",
        "\n",
        "    print(f\"[INFO] Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgSz-BFuR3yp"
      },
      "source": [
        "## Forward Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE0d5uqCR6Im"
      },
      "outputs": [],
      "source": [
        "def make_beta_schedule(T=1000, beta_start=1e-4, beta_end=0.02, device=None):\n",
        "    \"\"\"\n",
        "    Create a linear beta schedule and precompute useful terms.\n",
        "    Returns a dict containing tensors on `device`.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #creates a vector of size T that starts with beta_start and ends with beta_end\n",
        "    #this vector is used to later get the t multiplication of the different alpha_ts, please notice that t is the timestamps which ranges (0<t<T)\n",
        "    betas = torch.linspace(beta_start, beta_end, T, device=device, dtype=torch.float32) #this gets me the linearly spaced betas\n",
        "    alphas = 1.0 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)             # \\bar{\\alpha}_t\n",
        "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)          # sqrt(\\bar{\\alpha}_t)\n",
        "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)  # sqrt(1 - \\bar{\\alpha}_t)\n",
        "\n",
        "    return {\n",
        "        \"T\": T,\n",
        "        \"betas\": betas,\n",
        "        \"alphas\": alphas,\n",
        "        \"alphas_cumprod\": alphas_cumprod,\n",
        "        \"sqrt_alphas_cumprod\": sqrt_alphas_cumprod,\n",
        "        \"sqrt_one_minus_alphas_cumprod\": sqrt_one_minus_alphas_cumprod,\n",
        "        \"device\": device\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7q34WYpSIkA"
      },
      "outputs": [],
      "source": [
        "def inspect_beta_schedule(schedule, num_values=10):\n",
        "    \"\"\"\n",
        "    Prints out the first few values of the beta schedule and related terms\n",
        "    to help understand their behavior.\n",
        "\n",
        "    Args:\n",
        "        schedule: dict returned by make_beta_schedule\n",
        "        num_values: how many values to print from the start and end\n",
        "    \"\"\"\n",
        "    print(f\"Total timesteps (T): {schedule['T']}\")\n",
        "    print(f\"Device: {schedule['device']}\")\n",
        "    print(\"------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    def print_tensor_info(name, tensor):\n",
        "        tensor_cpu = tensor.detach().cpu()\n",
        "        print(f\"{name}:\")\n",
        "        print(\"Values:\", tensor_cpu[:num_values].numpy())\n",
        "        # print(\"  last values :\", tensor_cpu[-num_values:].numpy())\n",
        "        print(\"------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    print_tensor_info(\"betas\", schedule[\"betas\"])\n",
        "    print_tensor_info(\"alphas\", schedule[\"alphas\"])\n",
        "    print_tensor_info(\"alphas_cumprod\", schedule[\"alphas_cumprod\"])\n",
        "    print_tensor_info(\"sqrt_alphas_cumprod\", schedule[\"sqrt_alphas_cumprod\"])\n",
        "    print_tensor_info(\"sqrt_one_minus_alphas_cumprod\", schedule[\"sqrt_one_minus_alphas_cumprod\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T69QRIqsSO66"
      },
      "outputs": [],
      "source": [
        "def forward_diffusion_batch(x_start, schedule, timesteps=None):\n",
        "    \"\"\"\n",
        "    Vectorized forward diffusion for a batch.\n",
        "    Args:\n",
        "      - x_start: tensor [B, C, H, W] (float).\n",
        "      - schedule: dict returned by make_beta_schedule\n",
        "      - timesteps: optional LongTensor [B] of timesteps (0..T-1). If None, sampled uniformly.\n",
        "    Returns:\n",
        "      - x_t: noisy images tensor [B, C, H, W]\n",
        "      - noise: the Gaussian noise added [B, C, H, W]\n",
        "      - timesteps: LongTensor [B]\n",
        "    Notes:\n",
        "      - x_start can be normalized in form of [0,1] or [-1,1]; formula is invariant. Make sure model and loss use same range.\n",
        "    \"\"\"\n",
        "    device = schedule[\"device\"]\n",
        "    T = schedule[\"T\"]\n",
        "    sqrt_alphas_cumprod = schedule[\"sqrt_alphas_cumprod\"]   # shape [T]\n",
        "    sqrt_one_minus_alphas_cumprod = schedule[\"sqrt_one_minus_alphas_cumprod\"]\n",
        "\n",
        "    x_start = x_start.to(device)\n",
        "    B = x_start.shape[0]\n",
        "\n",
        "    if timesteps is None:\n",
        "        timesteps = torch.randint(1, T, (B,), device=device, dtype=torch.long)\n",
        "\n",
        "    # gather scalars per batch element and reshape to [B,1,1,1]\n",
        "    a_t = sqrt_alphas_cumprod[timesteps].view(B, 1, 1, 1)\n",
        "    b_t = sqrt_one_minus_alphas_cumprod[timesteps].view(B, 1, 1, 1)\n",
        "\n",
        "    noise = torch.randn_like(x_start, device=device)\n",
        "    x_t = a_t * x_start + b_t * noise\n",
        "\n",
        "    return x_t, noise, timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmgR4crUSSiC"
      },
      "outputs": [],
      "source": [
        "def show_sample_pair(x_clean, x_noisy, idx=0, title_clean=\"clean\", title_noisy=\"noisy\"):\n",
        "    \"\"\"\n",
        "    Helper to visualize a single sample (grayscale tensors).\n",
        "    Accepts tensors in either [0,1] or [-1,1] range.\n",
        "    \"\"\"\n",
        "    def to_display(img_tensor):\n",
        "        img = img_tensor.detach().cpu().squeeze().astype if False else None\n",
        "        # convert to CPU numpy\n",
        "        img_np = img_tensor.detach().cpu().numpy()\n",
        "        # channel-first (1,H,W) -> (H,W)\n",
        "        if img_np.ndim == 3 and img_np.shape[0] == 1:\n",
        "            img_np = img_np[0]\n",
        "        # detect range: if values < 0, assume [-1,1] -> convert to [0,1]\n",
        "        if img_np.min() < 0:\n",
        "            img_np = (img_np + 1.0) / 2.0\n",
        "        # clip to [0,1] for display\n",
        "        img_np = img_np.clip(0.0, 1.0)\n",
        "        return img_np\n",
        "\n",
        "    clean = to_display(x_clean[idx])\n",
        "    noisy = to_display(x_noisy[idx])\n",
        "\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(clean, cmap=\"gray\")\n",
        "    plt.title(title_clean)\n",
        "    plt.axis(\"off\")\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(noisy, cmap=\"gray\")\n",
        "    plt.title(title_noisy)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUlOO-G6SZ-A"
      },
      "outputs": [],
      "source": [
        "def _extract_images_from_batch(batch):\n",
        "    \"\"\"Return the first tensor in `batch` that looks like images with shape [B, C, H, W].\"\"\"\n",
        "    if isinstance(batch, torch.Tensor):\n",
        "        return batch\n",
        "    if isinstance(batch, (list, tuple)):\n",
        "        for item in batch:\n",
        "            if isinstance(item, torch.Tensor) and item.ndim == 4:\n",
        "                return item\n",
        "    if isinstance(batch, dict):\n",
        "        # common HF style: batch['image']\n",
        "        if \"image\" in batch and isinstance(batch[\"image\"], torch.Tensor):\n",
        "            return batch[\"image\"]\n",
        "        # otherwise return first tensor-like value with ndim==4\n",
        "        for v in batch.values():\n",
        "            if isinstance(v, torch.Tensor) and v.ndim == 4:\n",
        "                return v\n",
        "    raise ValueError(\"Could not find image tensor in batch. Expected shape [B,C,H,W].\")\n",
        "\n",
        "def _to_display_numpy(img_tensor):\n",
        "    \"\"\"\n",
        "    Convert a single image tensor [C,H,W] -> numpy [H,W] or [H,W,3] in range [0,1] for imshow.\n",
        "    Handles grayscale (C==1) and signed/unsigned ranges ([-1,1] or [0,1]).\n",
        "    \"\"\"\n",
        "    img = img_tensor.detach().cpu().numpy()\n",
        "    # if channel-first grayscale (1,H,W) -> (H,W)\n",
        "    if img.ndim == 3 and img.shape[0] == 1:\n",
        "        img = img[0]\n",
        "    elif img.ndim == 3 and img.shape[0] == 3:\n",
        "        img = np.transpose(img, (1,2,0))\n",
        "    # detect range: if values < -0.1 assume [-1,1], else assume [0,1]\n",
        "    if img.min() < -0.1:\n",
        "        img = (img + 1.0) / 2.0\n",
        "    # clip to [0,1]\n",
        "    img = np.clip(img, 0.0, 1.0)\n",
        "    return img\n",
        "\n",
        "def show_original_vs_noisy(\n",
        "    batch,\n",
        "    schedule,\n",
        "    forward_fn,\n",
        "    device=None,\n",
        "    num_images=4,\n",
        "    timesteps=None,\n",
        "    random_seed=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Display original vs noisy images for the first `num_images` in `batch`.\n",
        "\n",
        "    Args:\n",
        "      - batch: output from DataLoader (tensor, tuple/list, or dict)\n",
        "      - schedule: dict from make_beta_schedule(...)\n",
        "      - forward_fn: function(x_start, schedule, timesteps=None) -> (x_t, noise, timesteps)\n",
        "      - device: torch.device or None (will use schedule['device'] if None)\n",
        "      - num_images: how many items from batch to show\n",
        "      - timesteps: optional: int or list/1D-tensor of length B to force timesteps (otherwise sampled inside forward_fn)\n",
        "      - random_seed: optional seed for reproducibility of timesteps/noise\n",
        "    Returns:\n",
        "      - (x_clean, x_noisy, timesteps_used) as tensors (on schedule['device'])\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = schedule.get(\"device\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "    # Extract images tensor (expects shape [B,C,H,W])\n",
        "    images = _extract_images_from_batch(batch)\n",
        "    images = images.to(device)\n",
        "\n",
        "    B = images.shape[0]\n",
        "    num_show = min(num_images, B)\n",
        "\n",
        "    # prepare timesteps argument for forward_fn (if provided)\n",
        "    if timesteps is not None:\n",
        "        # allow int or list/array/tensor\n",
        "        if isinstance(timesteps, int):\n",
        "            timesteps_arg = torch.full((B,), timesteps, dtype=torch.long, device=device)\n",
        "        else:\n",
        "            timesteps_arg = torch.as_tensor(timesteps, dtype=torch.long, device=device)\n",
        "            if timesteps_arg.ndim == 0:\n",
        "                timesteps_arg = timesteps_arg.repeat(B)\n",
        "            if timesteps_arg.numel() != B:\n",
        "                raise ValueError(\"timesteps length must equal batch size if provided.\")\n",
        "    else:\n",
        "        timesteps_arg = None\n",
        "\n",
        "    # optional reproducibility\n",
        "    if random_seed is not None:\n",
        "        torch.manual_seed(random_seed)\n",
        "\n",
        "    # call forward diffusion (this returns x_t, noise, timesteps)\n",
        "    x_t, noise, timesteps_used = forward_fn(images, schedule, timesteps_arg)\n",
        "\n",
        "    # Move to CPU for plotting selected images\n",
        "    images_cpu = images[:num_show].detach().cpu()\n",
        "    x_t_cpu = x_t[:num_show].detach().cpu()\n",
        "    timesteps_cpu = timesteps_used[:num_show].detach().cpu()\n",
        "\n",
        "    # plot\n",
        "    fig, axes = plt.subplots(num_show, 2, figsize=(6, 3 * num_show))\n",
        "    if num_show == 1:\n",
        "        axes = np.array([axes])  # make indexing consistent\n",
        "\n",
        "    for i in range(num_show):\n",
        "        orig_np = _to_display_numpy(images_cpu[i])\n",
        "        noisy_np = _to_display_numpy(x_t_cpu[i])\n",
        "        ax_orig = axes[i, 0]\n",
        "        ax_noisy = axes[i, 1]\n",
        "\n",
        "        if orig_np.ndim == 2:\n",
        "            ax_orig.imshow(orig_np, cmap=\"gray\")\n",
        "            ax_noisy.imshow(noisy_np, cmap=\"gray\")\n",
        "        else:\n",
        "            ax_orig.imshow(orig_np)\n",
        "            ax_noisy.imshow(noisy_np)\n",
        "\n",
        "        ax_orig.set_title(f\"Original (idx {i})\")\n",
        "        ax_noisy.set_title(f\"Noisy (t={int(timesteps_cpu[i].item())})\")\n",
        "        ax_orig.axis(\"off\"); ax_noisy.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return images, x_t, timesteps_used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enKxPbeVSncg"
      },
      "source": [
        "## Training Model : benetraco/brain_ddpm_128 model from Hugging Face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfkqVQolUoZe"
      },
      "outputs": [],
      "source": [
        "def show_training_step_visuals(x_start, x_t, pred_noise, t, idx=0, clip_range=(0, 1)):\n",
        "    \"\"\"\n",
        "    Visualize clean, noisy, and denoised images for a sample in the batch.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import torch\n",
        "\n",
        "    def to_numpy_img(tensor_img):\n",
        "        img = tensor_img.detach().cpu().numpy()\n",
        "        # [C,H,W] → [H,W] for grayscale\n",
        "        if img.shape[0] == 1:\n",
        "            img = img[0]\n",
        "        img = np.clip(img, clip_range[0], clip_range[1])\n",
        "        if clip_range != (0, 1):\n",
        "            img = (img - clip_range[0]) / (clip_range[1] - clip_range[0])\n",
        "        return img\n",
        "\n",
        "    clean_img = to_numpy_img(x_start[idx])\n",
        "    noisy_img = to_numpy_img(x_t[idx])\n",
        "\n",
        "    # Get sqrt(alpha_cumprod) and sqrt(1 - alpha_cumprod) for this timestep\n",
        "    if isinstance(t, torch.Tensor):\n",
        "        t_val = t[idx] if t.ndim > 0 else t\n",
        "    else:\n",
        "        t_val = torch.tensor(t)\n",
        "\n",
        "    a_t = torch.sqrt(schedule[\"alphas_cumprod\"][t_val]).to(x_t.device)\n",
        "    b_t = torch.sqrt(1 - schedule[\"alphas_cumprod\"][t_val]).to(x_t.device)\n",
        "\n",
        "    # Correct denoising formula: x0 = (x_t - b_t * pred_noise) / a_t\n",
        "    denoised_tensor = (x_t[idx] - b_t * pred_noise[idx]) / a_t\n",
        "    denoised_img = to_numpy_img(denoised_tensor)\n",
        "\n",
        "    timestep_value = t_val.item()\n",
        "\n",
        "    titles = [\n",
        "        \"Original Clean Image\",\n",
        "        f\"Noisy Image (t={timestep_value})\",\n",
        "        \"Denoised Image\"\n",
        "    ]\n",
        "\n",
        "    images = [clean_img, noisy_img, denoised_img]\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for i, (img, title) in enumerate(zip(images, titles)):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(title)\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoiwbgDeUS2O"
      },
      "outputs": [],
      "source": [
        "def train_step(model, batch, schedule, optimizer, device):\n",
        "    \"\"\"\n",
        "    One training step:\n",
        "    - batch: tensor of shape [B, C, H, W], clean images normalized [0,1] or [-1,1]\n",
        "    - schedule: diffusion beta schedule dict\n",
        "    - optimizer: optimizer instance\n",
        "    - device: cuda or cpu\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    batch = batch.to(device)  # move to device\n",
        "\n",
        "    # Add noise at random timesteps (forward diffusion)\n",
        "    x_t, noise, timesteps = forward_diffusion_batch(batch, schedule)\n",
        "\n",
        "    # Predict noise from model\n",
        "    # Note: The model expects inputs normalized the same way as training\n",
        "    pred_noise = model(x_t, timesteps).sample\n",
        "\n",
        "    # Show visuals for the first image in the batch\n",
        "    show_training_step_visuals(batch, x_t, pred_noise, timesteps, idx=0, clip_range=(-1, 1))\n",
        "\n",
        "    # Compute MSE loss between predicted noise and true noise\n",
        "    loss = F.mse_loss(pred_noise, noise)\n",
        "\n",
        "    # loss.backward()\n",
        "    # optimizer.step()\n",
        "    # return loss.item()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4HwoorxSoEK"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    schedule,\n",
        "    device,\n",
        "    epochs=10,\n",
        "    lr=1e-4,\n",
        "    checkpoint_dir=\"./checkpoints\",\n",
        "    save_every=2,\n",
        "    log_every=10,\n",
        "    resume_checkpoint=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Train the diffusion model using HuggingFace Accelerator for mixed precision and device handling.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model (e.g., brain_ddpm_128).\n",
        "        train_loader: DataLoader providing training images.\n",
        "        schedule: diffusion beta schedule dictionary.\n",
        "        device: 'cuda' or 'cpu' (Accelerator will override this).\n",
        "        epochs: number of training epochs.\n",
        "        lr: learning rate.\n",
        "        checkpoint_dir: directory to save checkpoints.\n",
        "        save_every: save model every N epochs.\n",
        "        log_every: log the average loss every M batches.\n",
        "        resume_checkpoint: path to a checkpoint file to resume from.\n",
        "    \"\"\"\n",
        "\n",
        "    # Init Accelerator\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=\"fp16\",        # Enable mixed precision training\n",
        "        gradient_accumulation_steps=1  # Change if you want accumulation\n",
        "    )\n",
        "\n",
        "    # Make sure save directory exists\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Optimizer & Scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    # Resume from checkpoint if provided\n",
        "    start_epoch = 0\n",
        "    if resume_checkpoint is not None and os.path.isfile(resume_checkpoint):\n",
        "        accelerator.print(f\"Resuming from checkpoint: {resume_checkpoint}\")\n",
        "        checkpoint = torch.load(resume_checkpoint, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
        "        start_epoch = checkpoint[\"epoch\"] + 1\n",
        "\n",
        "    # Prepare objects with accelerator\n",
        "    model, optimizer, train_loader, scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_loader, scheduler\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            images = batch[0]  # take only images from (images, labels)\n",
        "\n",
        "            with accelerator.accumulate(model):\n",
        "                # Compute loss (train_step should return loss tensor now)\n",
        "                loss = train_step(model, images, schedule, optimizer, device)\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % log_every == 0 and accelerator.is_main_process:\n",
        "                accelerator.print(\n",
        "                    f\"[Epoch {epoch+1}/{epochs}] Step {batch_idx+1}/{len(train_loader)}, \"\n",
        "                    f\"Avg Loss: {epoch_loss/(batch_idx+1):.6f}\"\n",
        "                )\n",
        "\n",
        "        # Save checkpoint\n",
        "        if accelerator.is_main_process and ((epoch + 1) % save_every == 0):\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pth\")\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": accelerator.unwrap_model(model).state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"scheduler_state_dict\": scheduler.state_dict()\n",
        "            }, checkpoint_path)\n",
        "            accelerator.print(f\"Model saved to {checkpoint_path}\")\n",
        "\n",
        "        if accelerator.is_main_process:\n",
        "            accelerator.print(f\"Epoch {epoch+1} finished. Average Loss: {epoch_loss/len(train_loader):.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtUtnmsnU3as"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zihtqgvMU5Fi"
      },
      "outputs": [],
      "source": [
        "def load_latest_checkpoint(model, checkpoint_dir, device):\n",
        "    \"\"\"\n",
        "    Loads the latest checkpoint for the given model from checkpoint_dir.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model instance.\n",
        "        checkpoint_dir: Directory containing checkpoint files.\n",
        "        device: 'cuda' or 'cpu'.\n",
        "\n",
        "    Returns:\n",
        "        model, optimizer, scheduler, scaler, start_epoch\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(checkpoint_dir):\n",
        "        raise FileNotFoundError(f\"Checkpoint directory '{checkpoint_dir}' does not exist.\")\n",
        "\n",
        "    # List all checkpoint files\n",
        "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")]\n",
        "    if not checkpoints:\n",
        "        raise FileNotFoundError(\"No checkpoint files found in the directory.\")\n",
        "\n",
        "    # Sort by epoch number\n",
        "    checkpoints.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "    latest_checkpoint = checkpoints[-1]\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
        "\n",
        "    print(f\"Loading latest checkpoint: {checkpoint_path}\")\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # # Create empty placeholders if you don’t need optimizer/scheduler during testing\n",
        "    # optimizer = None\n",
        "    # scheduler = None\n",
        "    # scaler = None\n",
        "\n",
        "    return model, checkpoint[\"epoch\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF00c_HQVENu"
      },
      "outputs": [],
      "source": [
        "def compute_snr(clean, denoised):\n",
        "    \"\"\"\n",
        "    Compute Signal-to-Noise Ratio in dB.\n",
        "\n",
        "    Args:\n",
        "        clean: Tensor or numpy array, ground truth.\n",
        "        denoised: Tensor or numpy array, reconstructed output.\n",
        "\n",
        "    Returns:\n",
        "        snr_value: float, SNR in decibels.\n",
        "    \"\"\"\n",
        "    if isinstance(clean, torch.Tensor):\n",
        "        clean = clean.detach().cpu().numpy()\n",
        "    if isinstance(denoised, torch.Tensor):\n",
        "        denoised = denoised.detach().cpu().numpy()\n",
        "\n",
        "    noise = clean - denoised\n",
        "    signal_power = np.sum(clean ** 2)\n",
        "    noise_power = np.sum(noise ** 2)\n",
        "\n",
        "    if noise_power == 0:\n",
        "        return float(\"inf\")  # Perfect reconstruction\n",
        "    return 10 * np.log10(signal_power / noise_power)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD01vnxLiCLO"
      },
      "outputs": [],
      "source": [
        "def compute_ssim(clean, recon):\n",
        "    \"\"\"\n",
        "    Compute average SSIM between clean and reconstructed batch of grayscale images.\n",
        "    clean, recon: [B,C,H,W] tensors\n",
        "    \"\"\"\n",
        "    B, C, H, W = clean.shape\n",
        "    assert C == 1, \"SSIM is implemented here for grayscale only.\"\n",
        "\n",
        "    ssim_vals = []\n",
        "    for i in range(B):\n",
        "        ref = clean[i, 0].detach().cpu().numpy()\n",
        "        test = recon[i, 0].detach().cpu().numpy()\n",
        "        val = ssim(ref, test, data_range=ref.max() - ref.min())\n",
        "        ssim_vals.append(val)\n",
        "\n",
        "    return torch.tensor(np.mean(ssim_vals))  # return tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPBP_O7mhAV4"
      },
      "source": [
        "## NLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA0MjPcChB-p"
      },
      "outputs": [],
      "source": [
        "def fast_nlm_batch(noisy_batch, patch_size=3, patch_distance=5, h=0.8, fast_mode=True, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Apply fast Non-Local Means (NLM) denoising to a batch of grayscale images.\n",
        "    \"\"\"\n",
        "    B, C, H, W = noisy_batch.shape\n",
        "    assert C == 1, \"NLM is best suited for grayscale images (C=1).\"\n",
        "\n",
        "    denoised_list = []\n",
        "    for i in range(B):\n",
        "        img = noisy_batch[i, 0].detach().cpu().numpy()  # [H,W]\n",
        "\n",
        "        sigma_est = np.mean(estimate_sigma(img, channel_axis=None))\n",
        "\n",
        "        denoised = denoise_nl_means(\n",
        "            img,\n",
        "            h=h * sigma_est,\n",
        "            patch_size=patch_size,\n",
        "            patch_distance=patch_distance,\n",
        "            fast_mode=fast_mode,\n",
        "            channel_axis=None\n",
        "        )\n",
        "\n",
        "        denoised_list.append(torch.tensor(denoised, dtype=torch.float32).unsqueeze(0))  # [1,H,W]\n",
        "\n",
        "    return torch.stack(denoised_list, dim=0).to(device)  # [B,1,H,W]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkp2yYJdVKXw"
      },
      "outputs": [],
      "source": [
        "# def evaluate_model(model, test_loader, schedule, device=\"cpu\", num_batches=1):\n",
        "#     \"\"\"\n",
        "#     Evaluate model performance on noisy test data using SNR.\n",
        "\n",
        "#     Args:\n",
        "#         model: trained PyTorch model.\n",
        "#         test_loader: DataLoader for test dataset.\n",
        "#         schedule: diffusion beta schedule dict.\n",
        "#         device: device to run evaluation on.\n",
        "#         num_batches: number of batches to evaluate (can be 1 for single image batch).\n",
        "\n",
        "#     Returns:\n",
        "#         avg_snr: average SNR over evaluated batches.\n",
        "#     \"\"\"\n",
        "#     model.eval()\n",
        "#     snr_list = []\n",
        "#     ssim_list =[]\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch_idx, batch in enumerate(test_loader):\n",
        "#             if batch_idx >= num_batches:\n",
        "#                 break\n",
        "\n",
        "#             # Support DataLoader with or without labels\n",
        "#             images = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "#             images = images.to(device)\n",
        "\n",
        "#             # Add noise using forward diffusion\n",
        "#             x_t, noise, timesteps = forward_diffusion_batch(images, schedule)\n",
        "\n",
        "#             # Model predicts noise\n",
        "#             pred_noise = model(x_t, timesteps).sample\n",
        "\n",
        "#             # Reconstruct clean image estimate: x0_hat\n",
        "#             sqrt_alphas_cumprod = schedule[\"sqrt_alphas_cumprod\"]\n",
        "#             sqrt_one_minus_alphas_cumprod = schedule[\"sqrt_one_minus_alphas_cumprod\"]\n",
        "\n",
        "#             a_t = sqrt_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
        "#             b_t = sqrt_one_minus_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
        "#             reconstructed_images = (x_t - b_t * pred_noise) / a_t\n",
        "\n",
        "#             # Compute SNR\n",
        "#             snr_value = compute_snr(images, reconstructed_images)\n",
        "#             snr_list.append(float(snr_value))\n",
        "\n",
        "#             # Compute SNR\n",
        "#             ssim_value = compute_ssim(images, reconstructed_images)\n",
        "#             ssim_list.append(float(ssim_value))\n",
        "\n",
        "\n",
        "\n",
        "#     avg_snr = sum(snr_list) / len(snr_list) if snr_list else 0.0\n",
        "#     print(f\"Average SNR over {len(snr_list)} batch(es): {avg_snr:.4f} dB\")\n",
        "\n",
        "#     avg_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
        "#     print(f\"Average SSIM over {len(ssim_list)} batch(es): {avg_ssim:.4f}\")\n",
        "\n",
        "#     return avg_snr, avg_ssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54A_mwnImmnQ"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, schedule, device=\"cuda\", num_batches=1, use_nlm=False):\n",
        "    \"\"\"\n",
        "    Evaluate DDPM denoising performance (and optionally compare with NLM).\n",
        "    Computes SNR & SSIM for both methods.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = {\"ddpm\": {\"snr\": [], \"ssim\": []}}\n",
        "    if use_nlm:\n",
        "        results[\"nlm\"] = {\"snr\": [], \"ssim\": []}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(test_loader):\n",
        "            if batch_idx >= num_batches:\n",
        "                break\n",
        "\n",
        "            # Handle dataset with or without labels\n",
        "            if isinstance(batch, (list, tuple)):\n",
        "                images = batch[0].to(device)\n",
        "            else:\n",
        "                images = batch.to(device)\n",
        "\n",
        "            # Forward diffusion\n",
        "            x_t, noise, timesteps = forward_diffusion_batch(images, schedule)\n",
        "\n",
        "            # Model prediction\n",
        "            pred_noise = model(x_t, timesteps).sample\n",
        "\n",
        "\n",
        "            # Reconstruct clean image estimate: x0_hat\n",
        "            sqrt_alphas_cumprod = schedule[\"sqrt_alphas_cumprod\"]\n",
        "            sqrt_one_minus_alphas_cumprod = schedule[\"sqrt_one_minus_alphas_cumprod\"]\n",
        "\n",
        "            a_t = sqrt_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
        "            b_t = sqrt_one_minus_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
        "            recon_ddpm = (x_t - b_t * pred_noise) / a_t\n",
        "\n",
        "            # Metrics for DDPM\n",
        "            results[\"ddpm\"][\"snr\"].append(compute_snr(images, recon_ddpm).item())\n",
        "            results[\"ddpm\"][\"ssim\"].append(compute_ssim(images, recon_ddpm).item())\n",
        "\n",
        "            # Metrics for NLM (if enabled)\n",
        "            if use_nlm:\n",
        "                recon_nlm = fast_nlm_batch(x_t, device=device)\n",
        "                results[\"nlm\"][\"snr\"].append(compute_snr(images, recon_nlm).item())\n",
        "                results[\"nlm\"][\"ssim\"].append(compute_ssim(images, recon_nlm).item())\n",
        "\n",
        "    # Average results\n",
        "    for method in results:\n",
        "        results[method][\"snr\"] = np.mean(results[method][\"snr\"])\n",
        "        results[method][\"ssim\"] = np.mean(results[method][\"ssim\"])\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHqMX75OjhZf"
      },
      "outputs": [],
      "source": [
        "# def visualize_denoising(images, x_t, recon_ddpm, recon_nlm=None, num_images=4):\n",
        "#     \"\"\"\n",
        "#     Visualize original, noisy, DDPM denoised, and optionally NLM denoised images.\n",
        "#     \"\"\"\n",
        "#     num_images = min(num_images, images.shape[0])\n",
        "#     cols = 3 if recon_nlm is None else 4\n",
        "#     titles = [\"Original\", \"Noisy\", \"DDPM Denoised\"] + ([\"NLM Denoised\"] if recon_nlm is not None else [])\n",
        "\n",
        "#     fig, axs = plt.subplots(num_images, cols, figsize=(3*cols, 3*num_images))\n",
        "\n",
        "#     if num_images == 1:\n",
        "#         axs = [axs]  # ensure iterable\n",
        "\n",
        "#     for i in range(num_images):\n",
        "#         imgs_to_show = [images[i, 0], x_t[i, 0], recon_ddpm[i, 0]]\n",
        "#         if recon_nlm is not None:\n",
        "#             imgs_to_show.append(recon_nlm[i, 0])\n",
        "\n",
        "#         for j, img in enumerate(imgs_to_show):\n",
        "#             ax = axs[i, j] if num_images > 1 else axs[j]\n",
        "#             ax.imshow(img.detach().cpu().numpy(), cmap=\"gray\")\n",
        "#             ax.axis(\"off\")\n",
        "#             if i == 0:  # top row titles\n",
        "#                 ax.set_title(titles[j])\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_denoising(images, x_t, recon_ddpm, recon_nlm=None, timesteps=None, num_images=4):\n",
        "    \"\"\"\n",
        "    Visualize original, noisy, DDPM denoised, and optionally NLM denoised images.\n",
        "    Shows timestep t and SNR/SSIM values above each image.\n",
        "    \"\"\"\n",
        "    num_images = min(num_images, images.shape[0])\n",
        "    cols = 3 if recon_nlm is None else 4\n",
        "    titles = [\"Original\", \"Noisy\", \"DDPM Denoised\"] + ([\"NLM Denoised\"] if recon_nlm is not None else [])\n",
        "\n",
        "    fig, axs = plt.subplots(num_images, cols, figsize=(3*cols, 3*num_images))\n",
        "\n",
        "    if num_images == 1:\n",
        "        axs = [axs]\n",
        "\n",
        "    for i in range(num_images):\n",
        "        clean = images[i:i+1]\n",
        "        noisy = x_t[i:i+1]\n",
        "        ddpm = recon_ddpm[i:i+1]\n",
        "        nlm = recon_nlm[i:i+1] if recon_nlm is not None else None\n",
        "\n",
        "        # Compute metrics\n",
        "        snr_ddpm = compute_snr(clean, ddpm)\n",
        "        ssim_ddpm = compute_ssim(clean, ddpm)\n",
        "\n",
        "        if nlm is not None:\n",
        "            snr_nlm = compute_snr(clean, nlm)\n",
        "            ssim_nlm = compute_ssim(clean, nlm)\n",
        "\n",
        "        imgs_to_show = [clean[0, 0], noisy[0, 0], ddpm[0, 0]]\n",
        "        if nlm is not None:\n",
        "            imgs_to_show.append(nlm[0, 0])\n",
        "\n",
        "        for j, img in enumerate(imgs_to_show):\n",
        "            ax = axs[i, j] if num_images > 1 else axs[j]\n",
        "            ax.imshow(img.detach().cpu().numpy(), cmap=\"gray\")\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "            # Top row: fixed titles\n",
        "            if i == 0:\n",
        "                ax.set_title(titles[j], fontsize=11, pad=6)\n",
        "\n",
        "            # Add dynamic info above the image\n",
        "            if j == 1 and timesteps is not None:  # noisy image\n",
        "                t_val = timesteps[i].item()\n",
        "                ax.set_title(f\"{titles[j]}\\n(t={t_val})\", fontsize=10, pad=2)\n",
        "\n",
        "            if j == 2:  # DDPM\n",
        "                ax.set_title(f\"{titles[j]}\\nSNR={snr_ddpm:.2f}, SSIM={ssim_ddpm:.3f}\", fontsize=10, pad=2)\n",
        "\n",
        "            if j == 3 and nlm is not None:  # NLM\n",
        "                ax.set_title(f\"{titles[j]}\\nSNR={snr_nlm:.2f}, SSIM={ssim_nlm:.3f}\", fontsize=10, pad=2)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MkujXmBoo7VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2NC-PZUjld0"
      },
      "outputs": [],
      "source": [
        "def display_metrics_table(metrics_dict):\n",
        "    \"\"\"\n",
        "    Display evaluation metrics (SNR, SSIM) in a tabular format.\n",
        "    \"\"\"\n",
        "    headers = [\"Method\", \"SNR (dB)\", \"SSIM\"]\n",
        "    table = []\n",
        "\n",
        "    for method, vals in metrics_dict.items():\n",
        "        table.append([\n",
        "            method.upper(),\n",
        "            f\"{vals['snr']:.4f}\",\n",
        "            f\"{vals['ssim']:.4f}\"\n",
        "        ])\n",
        "\n",
        "    print(\"\\n=== Evaluation Metrics ===\")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"fancy_grid\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEuXiaaJQPkC"
      },
      "source": [
        "## Main Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLPnHyAeWdh_"
      },
      "outputs": [],
      "source": [
        "##################### Dataset Loading and visualization ###################################\n",
        "full_dataset, train_indices, test_indices = load_and_split_dataset()\n",
        "\n",
        "# Display 5 training images\n",
        "display_images(full_dataset, train_indices[:5])\n",
        "\n",
        "# Display 3 test images in a single row\n",
        "display_images(full_dataset, test_indices[:3], num_images=3, cols=3)\n",
        "\n",
        "# Show stats for first training image\n",
        "show_image_statistics(full_dataset, train_indices[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD4hvvB9WeWp"
      },
      "outputs": [],
      "source": [
        "################################### Training data Preprocessing ###################################\n",
        "\n",
        "# Preprocess and save training set\n",
        "train_tensor = preprocess_and_save_dataset(full_dataset, train_indices[0:128], target_size=(128, 128), normalize_type=\"signed\", save_path=\"/content/train_preprocessed.pt\")\n",
        "\n",
        "# Load later without preprocessing again\n",
        "train_tensor = torch.load(\"/content/train_preprocessed.pt\")\n",
        "\n",
        "# Show some preprocessed images\n",
        "display_preprocessed_images(train_tensor, num_images=16)\n",
        "\n",
        "# Show statistics for one preprocessed image\n",
        "show_preprocessed_statistics(train_tensor, index=0)\n",
        "\n",
        "################################### Testing data Preprocessing ###################################\n",
        "\n",
        "# Preprocess and save testing set\n",
        "test_tensor = preprocess_and_save_dataset(full_dataset, test_indices[0:32], target_size=(128, 128), normalize_type=\"signed\", save_path=\"/content/test_preprocessed.pt\")\n",
        "\n",
        "# Load later without preprocessing again\n",
        "test_tensor = torch.load(\"/content/test_preprocessed.pt\")\n",
        "\n",
        "# Show some preprocessed images\n",
        "display_preprocessed_images(test_tensor, num_images=2)\n",
        "\n",
        "# Show statistics for one preprocessed image\n",
        "show_preprocessed_statistics(test_tensor, index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlrdSvxmWenD"
      },
      "outputs": [],
      "source": [
        "################################### Definition of the  Model ###################################\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Load the pretrained DDPM pipeline\n",
        "model_id = \"benetraco/brain_ddpm_128\"\n",
        "pipeline = DDPMPipeline.from_pretrained(model_id)\n",
        "model = pipeline.unet.to(device)\n",
        "model.train()  # set to training mode\n",
        "\n",
        "\n",
        "################################### Start the model training ###################################\n",
        "train_loader, test_loader = get_data_loaders(train_path=\"/content/train_preprocessed.pt\",\n",
        "                                            test_path=\"/content/test_preprocessed.pt\",\n",
        "                                            batch_size=8)\n",
        "\n",
        "schedule = make_beta_schedule(T=100, beta_start=1e-4, beta_end=0.02, device=device)\n",
        "\n",
        "\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    schedule=schedule,\n",
        "    device=device,\n",
        "    epochs=10,\n",
        "    lr=2e-4,\n",
        "    checkpoint_dir=\"./ddpm_checkpoints\",\n",
        "    save_every=8,\n",
        "    log_every = 32,\n",
        "    resume_checkpoint=None  # or \"ddpm_checkpoints/model_epoch_20.pth\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOUabD4JWezg"
      },
      "outputs": [],
      "source": [
        "################################### Evaluation and comparing with NLM ###################################\n",
        "\n",
        "# Load latest model checkpoint\n",
        "model, checkpoint_epoch = load_latest_checkpoint(model, \"./ddpm_checkpoints\", device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThQQ891jlXfv"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test dataset\n",
        "test_schedule = make_beta_schedule(T=100, beta_start=1e-4, beta_end=0.001, device=device)\n",
        "\n",
        "# # Only DDPM\n",
        "# evaluate_model_1(model, test_loader, schedule, device=\"cuda\", num_batches=5)\n",
        "# Compare DDPM vs NLM\n",
        "results = evaluate_model(model, test_loader, test_schedule, device=\"cuda\", num_batches=20, use_nlm=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-bBJClmnegg"
      },
      "outputs": [],
      "source": [
        "# Show metrics in table\n",
        "display_metrics_table(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdZbzBzgnrB4"
      },
      "outputs": [],
      "source": [
        "# # Show some images\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         # Handle both possible cases: (images,) or images\n",
        "#         if isinstance(batch, (list, tuple)):\n",
        "#             images = batch[0].to(\"cuda\")\n",
        "#         else:\n",
        "#             images = batch.to(\"cuda\")\n",
        "\n",
        "#         x_t, noise, timesteps = forward_diffusion_batch(images, schedule)\n",
        "#         pred_noise = model(x_t, timesteps).sample\n",
        "#         recon_ddpm = x_t - pred_noise\n",
        "#         recon_nlm = fast_nlm_batch(x_t, device=\"cuda\")\n",
        "\n",
        "#         visualize_denoising(images, x_t, recon_ddpm, recon_nlm, num_images=3)\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        if isinstance(batch, (list, tuple)):\n",
        "            images = batch[0].to(\"cuda\")\n",
        "        else:\n",
        "            images = batch.to(\"cuda\")\n",
        "\n",
        "        x_t, noise, timesteps = forward_diffusion_batch(images, test_schedule)\n",
        "        pred_noise = model(x_t, timesteps).sample\n",
        "\n",
        "        # Reconstruct clean image estimate x0_hat\n",
        "        sqrt_alphas_cumprod = test_schedule[\"sqrt_alphas_cumprod\"]\n",
        "        sqrt_one_minus_alphas_cumprod = test_schedule[\"sqrt_one_minus_alphas_cumprod\"]\n",
        "        a_t = sqrt_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
        "        b_t = sqrt_one_minus_alphas_cumprod[timesteps].view(-1, 1, 1, 1)\n",
        "        recon_ddpm = (x_t - b_t * pred_noise) / a_t\n",
        "\n",
        "        recon_nlm = fast_nlm_batch(x_t, device=\"cuda\")\n",
        "\n",
        "        visualize_denoising(images, x_t, recon_ddpm, recon_nlm, timesteps=timesteps, num_images=7)\n",
        "        break"
      ],
      "metadata": {
        "id": "BUBfXtXTpBd9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}